{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykropivn/rpm2023_text_mining_workshop/blob/main/rpm_2023_text_mining_workshop_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00-S-ZSlgZrb"
      },
      "source": [
        "# Text Mining with Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18Py1d3kaKDu"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "2 + 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70uYF-dklgyz"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget"
      ],
      "metadata": {
        "id": "3SlruCP2Riuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-vCKQgLhQA4"
      },
      "outputs": [],
      "source": [
        "# Install missing packages\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfWlRwLwqQiD"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import zipfile\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDiMtusIavi9"
      },
      "outputs": [],
      "source": [
        "# Define the remote file to retrieve\n",
        "remote_url = (\n",
        "    'https://static.nhtsa.gov/nhtsa/downloads/CISS/2018/CISS_2018_CSV_files.zip'\n",
        ")\n",
        "\n",
        "# Define the local filename to save 2018 data\n",
        "local_file = 'CISS_2018_CSV_files.zip'\n",
        "\n",
        "# Make request for remote file data\n",
        "wget.download(remote_url, local_file)\n",
        "\n",
        "# Read CRASH.CSV file into pandas dataframe\n",
        "df_zip_18 = zipfile.ZipFile('CISS_2018_CSV_files.zip')\n",
        "raw_data_18 = pd.read_csv(df_zip_18.open('CRASH.CSV'), encoding='unicode_escape')\n",
        "\n",
        "# 2019\n",
        "remote_url = (\n",
        "    'https://static.nhtsa.gov/nhtsa/downloads/CISS/2019/CISS_2019_CSV_files.zip'\n",
        ")\n",
        "local_file = 'CISS_2019_CSV_files.zip'\n",
        "wget.download(remote_url, local_file)\n",
        "df_zip_19 = zipfile.ZipFile('CISS_2019_CSV_files.zip')\n",
        "raw_data_19 = pd.read_csv(df_zip_19.open('CRASH.CSV'), encoding='unicode_escape')\n",
        "\n",
        "# 2020\n",
        "remote_url = (\n",
        "    'https://static.nhtsa.gov/nhtsa/downloads/CISS/2020/CISS_2020_CSV_files.zip'\n",
        ")\n",
        "local_file = 'CISS_2020_CSV_files.zip'\n",
        "wget.download(remote_url, local_file)\n",
        "df_zip_20 = zipfile.ZipFile('CISS_2020_CSV_files.zip')\n",
        "raw_data_20 = pd.read_csv(df_zip_20.open('CRASH.csv'), encoding='unicode_escape')\n",
        "\n",
        "# 2021\n",
        "remote_url = (\n",
        "    'https://static.nhtsa.gov/nhtsa/downloads/CISS/2021/CISS_2021_CSV_files.zip'\n",
        ")\n",
        "local_file = 'CISS_2021_CSV_files.zip'\n",
        "wget.download(remote_url, local_file)\n",
        "df_zip_21 = zipfile.ZipFile('CISS_2021_CSV_files.zip')\n",
        "raw_data_21 = pd.read_csv(df_zip_21.open('CRASH.csv'), encoding='unicode_escape')\n",
        "\n",
        "# Concatinate all years and reindex 2018-2021\n",
        "raw_data = pd.concat([raw_data_18, raw_data_19, raw_data_20, raw_data_21])\n",
        "raw_data.reset_index(inplace=True, drop=True)\n",
        "\n",
        "raw_data.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18EQ_bItzktS"
      },
      "outputs": [],
      "source": [
        "# Let's check columns available\n",
        "raw_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L7LbPpzzGci"
      },
      "outputs": [],
      "source": [
        "# Select subset of columns for analysis\n",
        "case_text = raw_data[\n",
        "    [\n",
        "        'CASENUMBER',\n",
        "        'CRASHYEAR',\n",
        "        'CRASHMONTH',\n",
        "        'CRASHTIME',\n",
        "        'DAYOFWEEK',\n",
        "        'CATEGORY',\n",
        "        'CINJURED',\n",
        "        'CINJSEV',\n",
        "        'SUMMARY',\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Check top 3 rows\n",
        "case_text.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv3MB1XLNYzq"
      },
      "outputs": [],
      "source": [
        "# Check bottom 3 rows\n",
        "case_text.tail(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSmWEEfci3JG"
      },
      "outputs": [],
      "source": [
        "# There is one crash report in the dataset with a null case number\n",
        "case_text['CASENUMBER'][2169]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hglqxFxbYhOk"
      },
      "outputs": [],
      "source": [
        "# Let's drop it and re-index the dataframe \n",
        "case_text = case_text.dropna()\n",
        "case_text.reset_index(inplace=True, drop=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R91BVHM3lpEv"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxOoWoAzqMTm"
      },
      "outputs": [],
      "source": [
        "# Turning all warnings off\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9J8b8HGtAmX"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing - Lower case transformation\n",
        "case_text['text_low'] = case_text['SUMMARY'].astype(str).str.lower()\n",
        "case_text.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpfVKwFcs3VS"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing - tockenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "regexp = RegexpTokenizer('\\w+')\n",
        "\n",
        "case_text['text_token']=case_text['text_low'].apply(regexp.tokenize)\n",
        "case_text.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pZgKUPfs_I_"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing - stop words removal\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Make a list of english stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# Extend the list with your own custom stopwords\n",
        "my_stopwords = []\n",
        "stopwords.extend(my_stopwords)\n",
        "\n",
        "case_text['text_token_stop'] = case_text['text_token'].apply(\n",
        "    lambda x: [item for item in x if item not in stopwords]\n",
        ")\n",
        "case_text.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW9-0LuD6W7w"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing - stemming\n",
        "from nltk.stem.porter import *\n",
        "# from nltk.stem.snowball import *\n",
        "\n",
        "stemmer = PorterStemmer() # another option is SnowballStemmer(language='english'), which performs stemming for different languages\n",
        "\n",
        "case_text['text_token_stop_stem'] = case_text['text_token_stop'].apply(\n",
        "    lambda x: [stemmer.stem(y) for y in x]\n",
        ")\n",
        "case_text.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUF678uCt1lR"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing - filter by length (shorter than 2 symbols)\n",
        "case_text['text_string'] = case_text['text_token_stop_stem'].apply(\n",
        "    lambda x: ' '.join([item for item in x if len(item) > 2])\n",
        ")\n",
        "case_text.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR1r0ML8uL-w"
      },
      "outputs": [],
      "source": [
        "# Merging all words together and split them into tokens to perform frequency analysis\n",
        "all_words = ' '.join([word for word in case_text['text_string']])\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "tokenized_words = word_tokenize(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "MCQqWoovWICh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgEbQQ-3uTc6"
      },
      "outputs": [],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Creating FreqDist, keeping the 10 most common tokens\n",
        "fdist = FreqDist(tokenized_words).most_common(10)\n",
        "\n",
        "# Conversion to Pandas series via Python Dictionary for easier plotting\n",
        "fdist = pd.Series(dict(fdist))\n",
        "\n",
        "# Setting figure, ax into variables\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
        "all_plot = sns.barplot(x=fdist.values, y=fdist.index, orient='h', ax=ax)\n",
        "plt.title('Word Frequency Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2JDKaE-4q5C"
      },
      "outputs": [],
      "source": [
        "# Creating FreqDist, keeping the 20 least common tokens\n",
        "least_common = FreqDist(dict(FreqDist(tokenized_words).most_common()[-20:]))\n",
        "\n",
        "# Conversion to Pandas series via Python Dictionary for easier plotting\n",
        "least_common = pd.Series(dict(least_common))\n",
        "\n",
        "# Setting figure, ax into variables\n",
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "# Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
        "all_plot = sns.barplot(x=least_common.values, y=least_common.index, orient='h', ax=ax)\n",
        "plt.title('Word Frequency Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgRFcGjNuYTY"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing - filtering by overall word frequency\n",
        "fdist = FreqDist(tokenized_words)\n",
        "\n",
        "case_text['text_string_token']=case_text['text_string'].apply(regexp.tokenize)\n",
        "\n",
        "case_text['text_string_fdist'] = case_text['text_string_token'].apply(\n",
        "    lambda x: ' '.join(\n",
        "        [item for item in x if (fdist[item] >= 1 and fdist[item] <= 14000)]\n",
        "    )\n",
        ")\n",
        "case_text.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuPZmJYclx03"
      },
      "source": [
        "## Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "eQ268VLXYWNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq43_AeoeM3X"
      },
      "outputs": [],
      "source": [
        "# Word2Vec - A function to build corpus\n",
        "def build_corpus(data):\n",
        "    corpus = []\n",
        "    for doc in data.iteritems():\n",
        "        word_list = doc[1].split(' ')\n",
        "        corpus.append(word_list)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0OKoaCuedRj"
      },
      "outputs": [],
      "source": [
        "corpus = build_corpus(case_text['text_string_fdist'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "id": "68TyqN2AYmnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql6imlx9l2Oj"
      },
      "outputs": [],
      "source": [
        "model_10 = Word2Vec(corpus, size=10, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxW7x8fDEWRk"
      },
      "outputs": [],
      "source": [
        "# After training the word2vec model, we can obtain the word embedding directly from the training model as following\n",
        "model_10['deer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQT022W8FvjX"
      },
      "outputs": [],
      "source": [
        "model_10['anim']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUVsf0A7H5qI"
      },
      "outputs": [],
      "source": [
        "# Let's plot 5 word-vectors to see if the representation makes sense\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(\n",
        "    data=go.Heatmap(\n",
        "        z=[model_10['control'], model_10['path'], model_10['anim'], model_10['deer'], model_10['run']],\n",
        "        x=[\n",
        "            'dim1',\n",
        "            'dim2',\n",
        "            'dim3',\n",
        "            'dim4',\n",
        "            'dim5',\n",
        "            'dim6',\n",
        "            'dim7',\n",
        "            'dim8',\n",
        "            'dim9',\n",
        "            'dim10',\n",
        "        ],\n",
        "        y=['control', 'path', 'anim', 'deer', 'run'],\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Model understanding - We can obtain the word based on similarity\n",
        "sim_words = model_10.wv.most_similar('deer')\n",
        "sim_words"
      ],
      "metadata": {
        "id": "76ua7PUMbl--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Model understanding - We can obtain the word based on similarity\n",
        "sim_words = model_10.wv.most_similar('injur')\n",
        "sim_words"
      ],
      "metadata": {
        "id": "IHz3-KRGb2Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3j4XVHUiEfD"
      },
      "outputs": [],
      "source": [
        "#  Model understanding - function to visualize the model in 2D\n",
        "#  TSNE - t-distributed stochastic neighbor embedding\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.offline as py\n",
        "\n",
        "\n",
        "def tsne_plot(model):\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tokens = np.asarray(tokens)\n",
        "    # Dimensionality reduction\n",
        "    tsne_model = TSNE(perplexity=5, n_components=2, init='pca', n_iter=2500, random_state=123)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    traceTSNE = go.Scatter(\n",
        "        x=new_values[:, 0],\n",
        "        y=new_values[:, 1],\n",
        "        mode='markers+text',\n",
        "        text=labels,\n",
        "        marker=dict(\n",
        "            size=8,\n",
        "            colorscale='Jet',\n",
        "            showscale=False,\n",
        "            line=dict(width=2, color='rgb(255, 255, 255)'),\n",
        "            opacity=0.8,\n",
        "        ),\n",
        "    )\n",
        "    data = [traceTSNE]\n",
        "\n",
        "    layout = dict(\n",
        "        hovermode='closest',\n",
        "        yaxis=dict(zeroline=False),\n",
        "        xaxis=dict(zeroline=False),\n",
        "        autosize=False,\n",
        "        width=1600,\n",
        "        height=800,\n",
        "    )\n",
        "\n",
        "    fig = dict(data=data, layout=layout)\n",
        "    py.iplot(fig, filename='styled-scatter')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckZm6IlRtK8K"
      },
      "outputs": [],
      "source": [
        "# Plot 10 dimensions model in 2D\n",
        "tsne_plot(model_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGSbnkCVm9QD"
      },
      "outputs": [],
      "source": [
        "# Re-train word2vec model with 300 dimensions\n",
        "model_300 = Word2Vec(corpus, size=300, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReSj0BD6N8KK"
      },
      "outputs": [],
      "source": [
        "# Plot new vectors for the same words\n",
        "fig = go.Figure(\n",
        "    data=go.Heatmap(\n",
        "        z=[model_300['control'], model_300['path'], model_300['anim'], model_300['deer'], model_300['run']],\n",
        "        y=['control', 'path', 'anim', 'deer', 'run'],\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5Y7GMChhVWK"
      },
      "outputs": [],
      "source": [
        "#  Model understanding - We can obtain the word based on similarity\n",
        "sim_words = model_300.wv.most_similar('deer')\n",
        "sim_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_words = model_300.wv.most_similar('lefthand')\n",
        "sim_words"
      ],
      "metadata": {
        "id": "YpwAAw7Lay4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Model understanding - We can obtain the word based on similarity\n",
        "sim_words = model_300.wv.most_similar('injur')\n",
        "sim_words"
      ],
      "metadata": {
        "id": "E98M9Vp1cRdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtOmkmgNi7o9"
      },
      "outputs": [],
      "source": [
        "# Plot 300 dimensions model in 2D\n",
        "tsne_plot(model_300)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec Model"
      ],
      "metadata": {
        "id": "KLBpo5w3N6sf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrPzgB6QFxFP"
      },
      "outputs": [],
      "source": [
        "# Preparing data for Doc2Vec Model\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# Each document should have a list of words and a tag\n",
        "list_id = list(case_text['CASENUMBER'])\n",
        "list_def = list(case_text['text_string_fdist'])\n",
        "tagged_data = [\n",
        "    TaggedDocument(words=word_tokenize(term_def.lower()), tags=[list_id[i]])\n",
        "    for i, term_def in enumerate(list_def)\n",
        "]\n",
        "\n",
        "# Let's take a look at the example\n",
        "print(tagged_data[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyZ7i2jyK0uA"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Doc2Vec Model\n",
        "max_epochs = 50 # 250 iterations will take ~30 mins, ~8 sec per iteration\n",
        "vec_size = 100\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(vector_size=vec_size,\n",
        "                alpha=alpha,\n",
        "                min_alpha=0.00025,\n",
        "                dm=1,\n",
        "                min_count=2)\n",
        "  \n",
        "model.build_vocab(tagged_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09z1z0MgpWCz"
      },
      "outputs": [],
      "source": [
        "# Train the Doc2Vec Model\n",
        "for epoch in range(max_epochs):\n",
        "    if epoch % 10 == 0:\n",
        "        print('iteration {0}'.format(epoch))\n",
        "\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.epochs)\n",
        "    \n",
        "    model.alpha -= 0.0002\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save('d2v_do_v100_e50.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1giWVQ2bKgEA"
      },
      "outputs": [],
      "source": [
        "# Check model results - word similarity\n",
        "model.wv.similar_by_word('deer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2YM4IHyKplQ"
      },
      "outputs": [],
      "source": [
        "model.wv.similar_by_word('fatal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktEeocuxKuGE"
      },
      "outputs": [],
      "source": [
        "model.wv.similar_by_word('injuri')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv7iMqJUO2jO"
      },
      "outputs": [],
      "source": [
        "# Check model results - doc similarity\n",
        "similar_doc = model.docvecs.most_similar('1-32-2018-001-03')\n",
        "print('1-32-2018-001-03', 'is most similar to:')\n",
        "for i in range(len(similar_doc)):\n",
        "    print(similar_doc[i][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71EnIqSYPcwV"
      },
      "outputs": [],
      "source": [
        "case_text[case_text['CASENUMBER'].isin(['1-32-2018-001-03', '1-21-2019-027-08', '1-32-2020-157-05', '1-11-2020-064-07', '1-32-2020-063-09', '1-32-2020-094-04'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVwrBxmdLHV2"
      },
      "outputs": [],
      "source": [
        "# Check the top 10 most similar documents for all documents\n",
        "similarity_for_all =[]\n",
        "for i in range(len(tagged_data)): \n",
        "  similarity_for_all.append(model.docvecs.most_similar(tagged_data[i][1][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK96BGqjd4Eh"
      },
      "outputs": [],
      "source": [
        "similarity_for_all[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LFE0sHnIWd3"
      },
      "outputs": [],
      "source": [
        "# Reduce number of dimensions to 2D for visualization\n",
        "\n",
        "nrows = 12495 # Use lower number to use a subset of documents to speed things up, the total number of documents is 12495\n",
        "tsne = TSNE(perplexity=5, \n",
        "            n_components=2, \n",
        "            init='pca', \n",
        "            n_iter=1000, \n",
        "            random_state=123)\n",
        "tsne_d2v = tsne.fit_transform(model.docvecs.vectors_docs[:nrows])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnVKk_0rpyaP"
      },
      "outputs": [],
      "source": [
        "# Add SUMMARY as x,y into a dataframe\n",
        "tsne_d2v_df = pd.DataFrame(data=tsne_d2v, columns=['x', 'y'])  \n",
        "\n",
        "# Add other columns\n",
        "tsne_d2v_df['CATEGORY'] = case_text['CATEGORY'].values[:nrows]  \n",
        "tsne_d2v_df['CRASHYEAR'] = case_text['CRASHYEAR'].values[:nrows]\n",
        "tsne_d2v_df['CRASHMONTH'] = case_text['CRASHMONTH'].values[:nrows]\n",
        "tsne_d2v_df['CINJURED'] = case_text['CINJURED'].values[:nrows]\n",
        "tsne_d2v_df['CRASHTIME'] = case_text['CRASHTIME'].values[:nrows]\n",
        "tsne_d2v_df['DAYOFWEEK'] = case_text['DAYOFWEEK'].values[:nrows]\n",
        "tsne_d2v_df['CINJSEV'] = case_text['CINJSEV'].values[:nrows]\n",
        "tsne_d2v_df['SUMMARY'] = case_text['SUMMARY'].values[:nrows]\n",
        "\n",
        "# Add INJURED column\n",
        "tsne_d2v_df[\"INJURED\"] = np.where(tsne_d2v_df[\"CINJURED\"] == 0, 0, 1)\n",
        "\n",
        "# Add WINTER column\n",
        "seasons = {\n",
        "    1: \"1\",\n",
        "    2: \"1\",\n",
        "    3: \"0\",\n",
        "    4: \"0\",\n",
        "    5: \"0\",\n",
        "    6: \"0\",\n",
        "    7: \"0\",\n",
        "    8: \"0\",\n",
        "    9: \"0\",\n",
        "    10: \"0\",\n",
        "    11: \"0\",\n",
        "    12: \"1\",\n",
        "}\n",
        "tsne_d2v_df[\"WINTER\"] = tsne_d2v_df[\"CRASHMONTH\"].apply(lambda x: seasons[x])\n",
        "\n",
        "# Add NIGHT column\n",
        "tsne_d2v_df[\"NIGHT\"] = tsne_d2v_df[\"CRASHTIME\"].apply(\n",
        "    lambda x: \"1\" if \"00:00\" <= x <= \"06:00\" or \"20:00\" <= x <= \"23:00\" else \"0\"\n",
        ")\n",
        "\n",
        "# Add weekend column\n",
        "tsne_d2v_df[\"WEEKEND\"] = tsne_d2v_df[\"DAYOFWEEK\"].apply(\n",
        "    lambda x: \"1\" if 6 <= x <= 7 else \"0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADuaWkoV2R6T"
      },
      "outputs": [],
      "source": [
        "# Check top 3 rows\n",
        "tsne_d2v_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = tsne_d2v_df.hist(column='CRASHYEAR')"
      ],
      "metadata": {
        "id": "sc_73j6ZpaeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YXzHfGmwgOj"
      },
      "outputs": [],
      "source": [
        "from bokeh.models import ColumnDataSource, LabelSet, HoverTool\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import show, output_notebook\n",
        "from bokeh.palettes import Magma, Inferno, Plasma, Viridis, OrRd3, Turbo # You can try different pallets\n",
        "from bokeh.models import LinearColorMapper\n",
        "\n",
        "output_notebook()\n",
        "\n",
        "exp_cmap = LinearColorMapper(palette='Turbo256', \n",
        "                             low = min(tsne_d2v_df.CRASHYEAR.astype(int)), \n",
        "                             high = max(tsne_d2v_df.CRASHYEAR.astype(int)))\n",
        "\n",
        "source = ColumnDataSource(tsne_d2v_df)\n",
        "\n",
        "TOOLS='hover,crosshair,pan,box_zoom,undo,reset'\n",
        "TOOLTIPS = [\n",
        "    ('SUMMARY', '@SUMMARY'),\n",
        "]\n",
        "\n",
        "p = figure(plot_height =600, plot_width = 1200, tools=TOOLS, tooltips=TOOLTIPS)\n",
        "p.scatter(x='x', y='y', source=source, size=8, fill_color={'field':'CRASHYEAR', 'transform':exp_cmap}, alpha=0.5, legend_field='CRASHYEAR')\n",
        "\n",
        "show(p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = tsne_d2v_df.hist(column='CRASHMONTH', bins=12)"
      ],
      "metadata": {
        "id": "KCZqmcZcpf42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_notebook()\n",
        "\n",
        "exp_cmap = LinearColorMapper(palette='Turbo256', \n",
        "                             low = min(tsne_d2v_df.CRASHMONTH.astype(int)), \n",
        "                             high = max(tsne_d2v_df.CRASHMONTH.astype(int)))\n",
        "\n",
        "source = ColumnDataSource(tsne_d2v_df)\n",
        "\n",
        "p = figure(plot_height = 600, plot_width = 1200, tools=TOOLS, tooltips=TOOLTIPS)\n",
        "p.scatter(x='x', y='y', source=source, size=8, fill_color={'field':'CRASHMONTH', 'transform':exp_cmap}, alpha=0.5, legend_field='CRASHMONTH')\n",
        "\n",
        "show(p)"
      ],
      "metadata": {
        "id": "NlSn3wpm_eFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPW+ty/RHZdM5N6hQTM8AzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}